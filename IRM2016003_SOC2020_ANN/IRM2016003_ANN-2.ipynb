{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Assignment - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Using two input one output X-NOR data , train a NN using Back Propagation Algorithm. Don't use any built in function for Back Propagation. Also explain how will you test the network. All codes are to be written in Python language.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "expected_output = np.array([[0],[1],[1],[0]])\n",
    "epochs = 10000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))\n",
    "hidden_bias    = np.random.uniform(size=(1,hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
    "output_bias    = np.random.uniform(size=(1,outputLayerNeurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    \n",
    "    #Forward Propagation\n",
    "    hidden_layer_activation  =  np.dot(inputs,hidden_weights)\n",
    "    hidden_layer_activation  += hidden_bias\n",
    "    hidden_layer_output      =  sigmoid(hidden_layer_activation)\n",
    "\n",
    "    output_layer_activation = np.dot(hidden_layer_output,output_weights)\n",
    "    output_layer_activation += output_bias\n",
    "    predicted_output        = sigmoid(output_layer_activation)\n",
    "    \n",
    "    #Back Propagate error\n",
    "    error              = expected_output - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "    d_hidden_layer     = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    # Weights and Biases\n",
    "    output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "    output_bias    += np.sum(d_predicted_output, axis = 0, keepdims = True) * lr\n",
    "    hidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "    hidden_bias    += np.sum(d_hidden_layer, axis = 0, keepdims = True) * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden weights: [3.6443092  5.90947766] [3.63558156 5.86246043]\n",
      "Final hidden bias: [-5.57224883 -2.45850127]\n",
      "Final output weights: [-8.16701468] [7.46874619]\n",
      "Final output bias: [-3.33551729]\n",
      "Output after training: [0.05854295] [0.94602102] [0.9461649] [0.05836236]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "print(\"Output after training: \",end='')\n",
    "print(*predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``` Hence the output represents [0][1][1][0] which is the output of XOR gate```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the network, we will input a value slightly more or less than 0 and 1. \n",
    "\n",
    "By doing so, we can evaluate whether our neural network actually predicts the values correctly for slightly deviated values or not. This tests the robustness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
